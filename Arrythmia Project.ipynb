{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently, I was reviewing Andrew Ng's team's work(https://stanfordmlgroup.github.io/projects/ecg/) on heart arrhythmia detector with convolutional neural networks (CNN). I found this quite fascinating especially with emergence of wearable products (e.g. Apple Watch and portable EKG machines) that are capable of monitoring your heart while at home. As such, I was curious how to build a machine learning algorithm that could detect abnormal heart beats. Here we will use an ECG signal (continuous electrical measurement of the heart) and train 3 neural networks to predict heart arrythmias: dense neural network, CNN, and LSTM.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the MIH-BIH Arrythmia dataset from https://physionet.org/content/mitdb/1.0.0/. This is a dataset with 48 half-hour two-channel ECG recordings measured at 360 Hz. The recordings have annotations from cardiologists for each heart beat. The symbols for the annotations can be found at https://archive.physionet.org/physiobank/annotations.shtml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict if a heart beat from the first ECG signal has an arrhythmia for each 6 second window centered on the peak of the heart beat. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the problem, we will assume that a QRS detector is capable of automatically identifying the peak of each heart beat. We will ignore any non-beat annotations and any heart beats in the first or last 3 seconds of the recording due to reduced data. We will use a window of 6 seconds so we can compare the current beat to beats just before and after. This decision was based after talking to a physician who said it is easier to identify if you have something to compare it to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data must be downloaded and path provided\n",
    "data_path = r'C:\\Users\\bathu\\Desktop\\mit-bih-arrhythmia-database-1.0.0/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of patients\n",
    "pts = ['100','101','102','103','104','105','106','107',\n",
    "       '108','109','111','112','113','114','115','116',\n",
    "       '117','118','119','121','122','123','124','200',\n",
    "       '201','202','203','205','207','208','209','210',\n",
    "       '212','213','214','215','217','219','220','221',\n",
    "       '222','223','228','230','231','232','233','234']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use a pypi package wfdb for loading the ecg and annotations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load all the annotations and see the distribution of heart beat types across all files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for pt in pts:\n",
    "    file = data_path + pt\n",
    "    annotation = wfdb.rdann(file, 'atr')\n",
    "    sym = annotation.symbol\n",
    "    \n",
    "    values, counts = np.unique(sym, return_counts=True)\n",
    "    df_sub = pd.DataFrame({'sym':values, 'val':counts, 'pt':[pt]*len(counts)})\n",
    "    df = pd.concat([df, df_sub],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('sym').val.sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of nonbeat and abnormal\n",
    "nonbeat = ['[','!',']','x','(',')','p','t','u','`',\n",
    "           '\\'','^','|','~','+','s','T','*','D','=','\"','@','Q','?']\n",
    "abnormal = ['L','R','V','/','A','f','F','j','a','E','J','e','S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break into normal, abnormal or nonbeat\n",
    "df['cat'] = -1\n",
    "df.loc[df.sym == 'N','cat'] = 0\n",
    "df.loc[df.sym.isin(abnormal), 'cat'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('cat').val.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function for loading a single patient's signals and annotations. Note the annotation values are the indices of the signal array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ecg(file):\n",
    "    # load the ecg\n",
    "    # example file: 'mit-bih-arrhythmia-database-1.0.0/101'\n",
    "    \n",
    "    # load the ecg\n",
    "    record = wfdb.rdrecord(file)\n",
    "    # load the annotation\n",
    "    annotation = wfdb.rdann(file, 'atr')\n",
    "    \n",
    "    # extract the signal\n",
    "    p_signal = record.p_signal\n",
    "    \n",
    "    # verify frequency is 360\n",
    "    assert record.fs == 360, 'sample freq is not 360'\n",
    "    \n",
    "    # extract symbols and annotation index\n",
    "    atr_sym = annotation.symbol\n",
    "    atr_sample = annotation.sample\n",
    "    \n",
    "    return p_signal, atr_sym, atr_sample "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out what abnormal beats are in a patient's ecg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = data_path + pts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_signal, atr_sym, atr_sample = load_ecg(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, counts = np.unique(sym, return_counts=True)\n",
    "for v,c in zip(values, counts):\n",
    "    print(v,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a plot of these, zooming in on one of the abnormal beats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get abnormal beat index\n",
    "ab_index = [b for a,b in zip(atr_sym,atr_sample) if a in abnormal][:10]\n",
    "ab_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(p_signal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = ab_index[1]-20800\n",
    "right = ab_index[1]+20800\n",
    "\n",
    "plt.plot(x[left:right],p_signal[left:right,0],'-',label='ecg',)\n",
    "plt.plot(x[atr_sample],p_signal[atr_sample,0],'go',label ='normal')\n",
    "plt.plot(x[ab_index],p_signal[ab_index,0],'ro',label='abnormal')\n",
    "\n",
    "plt.xlim(left,right)\n",
    "plt.ylim(p_signal[left:right].min()-0.05,p_signal[left:right,0].max()+0.05)\n",
    "plt.xlabel('time index')\n",
    "plt.ylabel('ECG signal')\n",
    "plt.legend(bbox_to_anchor = (1.04,1), loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a dataset that is centered on beats with +- 3 seconds before and after. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(pts, num_sec, fs, abnormal):\n",
    "    # function for making dataset ignoring non-beats\n",
    "    # input:\n",
    "    # pts - list of patients\n",
    "    # num_sec = number of seconds to include before and after the beat\n",
    "    # fs = frequency\n",
    "    # output: \n",
    "    #   X_all = signal (nbeats , num_sec * fs columns)\n",
    "    #   Y_all = binary is abnormal (nbeats, 1)\n",
    "    #   sym_all = beat annotation symbol (nbeats,1)\n",
    "    \n",
    "    # initialize numpy arrays\n",
    "    num_cols = 2*num_sec * fs\n",
    "    X_all = np.zeros((1,num_cols))\n",
    "    Y_all = np.zeros((1,1))\n",
    "    sym_all = []\n",
    "    \n",
    "    # list to keep track of number of beats across patients\n",
    "    max_rows = []\n",
    "    \n",
    "    for pt in pts:\n",
    "        file = data_path + pt\n",
    "        \n",
    "        p_signal, atr_sym, atr_sample = load_ecg(file)\n",
    "        \n",
    "        # grab the first signal\n",
    "        p_signal = p_signal[:,0]\n",
    "        \n",
    "        # make df to exclude the nonbeats\n",
    "        df_ann = pd.DataFrame({'atr_sym':atr_sym,\n",
    "                              'atr_sample':atr_sample})\n",
    "        df_ann = df_ann.loc[df_ann.atr_sym.isin(abnormal + ['N'])]\n",
    "        \n",
    "        X,Y,sym = build_XY(p_signal,df_ann, num_cols, abnormal)\n",
    "        sym_all = sym_all+sym\n",
    "        max_rows.append(X.shape[0])\n",
    "        X_all = np.append(X_all,X,axis = 0)\n",
    "        Y_all = np.append(Y_all,Y,axis = 0)\n",
    "    # drop the first zero row\n",
    "    X_all = X_all[1:,:]\n",
    "    Y_all = Y_all[1:,:]\n",
    "    \n",
    "    # check sizes make sense\n",
    "    assert np.sum(max_rows) == X_all.shape[0], 'number of X, max_rows rows messed up'\n",
    "    assert Y_all.shape[0] == X_all.shape[0], 'number of X, Y rows messed up'\n",
    "    assert Y_all.shape[0] == len(sym_all), 'number of Y, sym rows messed up'\n",
    "\n",
    "    return X_all, Y_all, sym_all\n",
    "\n",
    "\n",
    "\n",
    "def build_XY(p_signal, df_ann, num_cols, abnormal):\n",
    "    # this function builds the X,Y matrices for each beat\n",
    "    # it also returns the original symbols for Y\n",
    "    \n",
    "    num_rows = len(df_ann)\n",
    "\n",
    "    X = np.zeros((num_rows, num_cols))\n",
    "    Y = np.zeros((num_rows,1))\n",
    "    sym = []\n",
    "    \n",
    "    # keep track of rows\n",
    "    max_row = 0\n",
    "\n",
    "    for atr_sample, atr_sym in zip(df_ann.atr_sample.values,df_ann.atr_sym.values):\n",
    "\n",
    "        left = max([0,(atr_sample - num_sec*fs) ])\n",
    "        right = min([len(p_signal),(atr_sample + num_sec*fs) ])\n",
    "        x = p_signal[left: right]\n",
    "        if len(x) == num_cols:\n",
    "            X[max_row,:] = x\n",
    "            Y[max_row,:] = int(atr_sym in abnormal)\n",
    "            sym.append(atr_sym)\n",
    "            max_row += 1\n",
    "    X = X[:max_row,:]\n",
    "    Y = Y[:max_row,:]\n",
    "    return X,Y,sym\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: split on patients not on samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by processing all of our patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sec = 3\n",
    "fs = 360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all, Y_all, sym_all = make_dataset(pts, num_sec, fs, abnormal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we naively just decided to randomly split our data by samples into a train and validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, Y_all, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here X_valid and y_valid is the test data. i.e x_test and y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to build our first dense NN. We will do this in Keras for simplicity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from kerastuner.tuners import RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform Hyperparameter Optimization\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding, Flatten, LeakyReLU, BatchNormalization, Dropout\n",
    "from keras.activations import relu, sigmoid\n",
    "\n",
    "\n",
    "\n",
    "def create_model(layers, activation):\n",
    "    model = Sequential()\n",
    "    for i, nodes in enumerate(layers):\n",
    "        if i==0:\n",
    "            model.add(Dense(nodes,input_dim=X_train.shape[1]))\n",
    "            model.add(Activation(activation))\n",
    "            model.add(Dropout(0.3))\n",
    "        else:\n",
    "            model.add(Dense(nodes))\n",
    "            model.add(Activation(activation))\n",
    "            model.add(Dropout(0.3))\n",
    "            \n",
    "    model.add(Dense(units = 1, kernel_initializer= 'glorot_uniform', activation = 'sigmoid')) # Note: no activation beyond this point\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [[20], [40, 20], [45, 30, 15]]\n",
    "activations = ['sigmoid', 'relu']\n",
    "param_grid = dict(layers=layers, activation=activations, batch_size = [128, 256], epochs=[30])\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid,cv=5)\n",
    "\n",
    "grid_result = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout:for each batch, a random portion of the outputs are nullified in order to avoid strong dependencies between portions of adjacent layers. This technique is similar to boosting techniques on decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the same model\n",
    "# lets test out relu (a different activation function) and add drop out (for regularization)\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation = 'relu', input_dim = X_train.shape[1]))\n",
    "model.add(Dropout(rate = 0.25))\n",
    "model.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model - use categorical crossentropy, and the adam optimizer\n",
    "model.compile(\n",
    "                loss = 'binary_crossentropy',\n",
    "                optimizer = 'adam',\n",
    "                metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_size: Integer or None. Number of samples per batch of computation. If unspecified, batch_size will default to 32.\n",
    "\n",
    "verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar.\n",
    "\n",
    "X_train: input data\n",
    "\n",
    "y_train : traget data\n",
    "\n",
    "epochs : An epoch is an iteration over the entire x and y data provided.The model is not trained for a number of iterations given by epochs, but merely until the epoch of index epochs is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size = 32, epochs= 5, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "def calc_prevalence(y_actual):\n",
    "    return (sum(y_actual)/len(y_actual))\n",
    "def calc_specificity(y_actual, y_pred, thresh):\n",
    "    # calculates specificity\n",
    "    return sum((y_pred < thresh) & (y_actual == 0)) /sum(y_actual ==0)\n",
    "def print_report(y_actual, y_pred, thresh):\n",
    "    \n",
    "    auc = roc_auc_score(y_actual, y_pred)\n",
    "    accuracy = accuracy_score(y_actual, (y_pred > thresh))\n",
    "    recall = recall_score(y_actual, (y_pred > thresh))\n",
    "    precision = precision_score(y_actual, (y_pred > thresh))\n",
    "    specificity = calc_specificity(y_actual, y_pred, thresh)\n",
    "    print('AUC:%.3f'%auc)\n",
    "    print('accuracy:%.3f'%accuracy)\n",
    "    print('recall:%.3f'%recall)\n",
    "    print('precision:%.3f'%precision)\n",
    "    print('specificity:%.3f'%specificity)\n",
    "    print('prevalence:%.3f'%calc_prevalence(y_actual))\n",
    "    print(' ')\n",
    "    return auc, accuracy, recall, precision, specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds_dense = model.predict_proba(X_train,verbose = 1)\n",
    "y_valid_preds_dense = model.predict_proba(X_valid,verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = (sum(y_train)/len(y_train))[0]\n",
    "thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train');\n",
    "print_report(y_train, y_train_preds_dense, thresh)\n",
    "print('Valid');\n",
    "print_report(y_valid, y_valid_preds_dense, thresh);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing! Not that hard! But wait, will this work on new patients? Perhaps not if each patient has a unique heart signature. Technically the same patient can show up in both the training and validation sets. This means that we may have accidentally leaked information across the datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try this again by splitting on patients instead of samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed( 42 )\n",
    "pts_train = random.sample(pts, 38)\n",
    "pts_test = [pt for pt in pts if pt not in pts_train]\n",
    "print(len(pts_train), len(pts_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, sym_train = make_dataset(pts_train, num_sec, fs, abnormal)\n",
    "X_valid, y_valid, sym_valid = make_dataset(pts_valid, num_sec, fs, abnormal)\n",
    "print(X_train.shape, y_train.shape, len(sym_train))\n",
    "print(X_valid.shape, y_valid.shape, len(sym_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the same model\n",
    "# lets test out relu (a different activation function) and add drop out (for regularization)\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation = 'relu', input_dim = X_train.shape[1]))\n",
    "model.add(Dropout(rate = 0.25))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=32,\n",
    "                                        max_value=512,\n",
    "                                        step=32),\n",
    "                           activation='relu'))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(\n",
    "            hp.Choice('learning_rate',\n",
    "                      values=[1e-2, 1e-3, 1e-4])),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# compile the model - use categorical crossentropy, and the adam optimizer\n",
    "model.compile(\n",
    "                loss = 'binary_crossentropy',\n",
    "                optimizer = 'adam',\n",
    "                metrics = ['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size = 32, epochs= 5, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds_dense = model.predict_proba(X_train,verbose = 1)\n",
    "y_valid_preds_dense = model.predict_proba(X_valid,verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = (sum(y_train)/len(y_train))[0]\n",
    "thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train');\n",
    "print_report(y_train, y_train_preds_dense, thresh)\n",
    "print('Valid');\n",
    "print_report(y_valid, y_valid_preds_dense, thresh);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation score is much different now! Makes sense since we had data leakage before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2: learning curve can tells us we should get more data! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the overfitting between training and validation. Let's make a simple learning curve to see if we should go collect more data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucs_train = []\n",
    "aucs_valid = []\n",
    "\n",
    "n_pts = [1,18,36]\n",
    "for n_pt in n_pts:\n",
    "    \n",
    "    print(n_pt)\n",
    "    pts_sub = pts_train[:n_pt]\n",
    "    X_sub, y_sub, sym_sub = make_dataset(pts_sub, num_sec, fs,abnormal)\n",
    "\n",
    "    # build the same model\n",
    "    # lets test out relu (a different activation function) and add drop out (for regularization)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation = 'relu', input_dim = X_train.shape[1]))\n",
    "    model.add(Dropout(rate = 0.25))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "    # compile the model - use categorical crossentropy, and the adam optimizer\n",
    "    model.compile(\n",
    "                    loss = 'binary_crossentropy',\n",
    "                    optimizer = 'adam',\n",
    "                    metrics = ['accuracy'])\n",
    "\n",
    "    model.fit(X_sub, y_sub, batch_size = 32, epochs= 5, verbose = 0)\n",
    "    y_sub_preds_dense = model.predict_proba(X_sub,verbose = 0)\n",
    "    y_valid_preds_dense = model.predict_proba(X_valid,verbose = 0)\n",
    "    \n",
    "    auc_train = roc_auc_score(y_sub, y_sub_preds_dense)\n",
    "    auc_valid = roc_auc_score(y_valid, y_valid_preds_dense)\n",
    "    print('-',auc_train, auc_valid)\n",
    "    aucs_train.append(auc_train)\n",
    "    aucs_valid.append(auc_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(n_pts, aucs_train, 'o-',label = 'Train')\n",
    "plt.plot(n_pts, aucs_valid, 'o-',label = 'Valid')\n",
    "plt.xlabel('Number Training Pts')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend(bbox_to_anchor = (1.04,1), loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More data appears to add extra value to the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  3: test multiple types of deep learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by making a CNN. Here we will use a 1 dimensional CNN (as opposed to the 2D CNN for images). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A CNN is a special type of deep learning algorithm which uses a set of filters and the convolution operator to reduce the number of parameters. This algorithm sparked the state-of-the-art techniques for image classification. Essentially, the way this works for 1D CNN is to take a filter (kernel) of size `kernel_size` starting with the first time stamp. The convolution operator takes the filter and multiplies each element against the first `kernel_size` time steps. These products are then summed for the first cell in the next layer of the neural network. The filter then moves over by `stride` time steps and repeats. The default `stride` in Keras is 1, which we will use. In image classification, most people use `padding` which allows you pick up some features on the edges of the image by adding 'extra' cells, we will use the default padding which is 0. The output of the convolution is then multiplied by a set of weights W and added to a bias b and then passed through a non-linear activation function as in dense neural network. You can then repeat this with addition CNN layers if desired. Here we will use Dropout which is a technique for reducing overfitting by randomly removing some nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features = 1]\n",
    "X_train_cnn = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_valid_cnn = np.reshape(X_valid, (X_valid.shape[0], X_valid.shape[1], 1))\n",
    "\n",
    "print(X_train_cnn.shape)\n",
    "print(X_valid_cnn.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters = 128, kernel_size = 5, activation = 'relu', input_shape = (2160,1)))\n",
    "model.add(Flatten())model.add(Dropout(rate = 0.25))\n",
    "\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "# compile the model - use categorical crossentropy, and the adam optimizer\n",
    "model.compile(\n",
    "                loss = 'binary_crossentropy',\n",
    "                optimizer = 'adam',\n",
    "                metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_cnn, y_train, batch_size = 32, epochs= 2, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds_cnn = model.predict_proba(X_train_cnn,verbose = 1)\n",
    "y_valid_preds_cnn = model.predict_proba(X_valid_cnn,verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train');\n",
    "print_report(y_train, y_train_preds_cnn, thresh)\n",
    "print('Valid');\n",
    "print_report(y_valid, y_valid_preds_cnn, thresh);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cnn.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(64, input_shape=(X_train_cnn.shape[1], X_train_cnn.shape[2]))))\n",
    "model.add(Dropout(rate = 0.25))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(\n",
    "                loss = 'binary_crossentropy',\n",
    "                optimizer = 'adam',\n",
    "                metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce dataset to make this feasible for weekend project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_cnn[:10000], y_train[:10000], batch_size = 32, epochs= 1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds_lstm = model.predict_proba(X_train_cnn[:10000],verbose = 1)\n",
    "y_valid_preds_lstm = model.predict_proba(X_valid_cnn,verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train');\n",
    "print_report(y_train[:10000], y_train_preds_lstm, thresh)\n",
    "print('Valid');\n",
    "print_report(y_valid, y_valid_preds_lstm, thresh);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "\n",
    "fpr_valid_cnn, tpr_valid_cnn, t_valid_cnn = roc_curve(y_valid, y_valid_preds_cnn)\n",
    "auc_valid_cnn = roc_auc_score(y_valid, y_valid_preds_cnn)\n",
    "\n",
    "fpr_valid_dense, tpr_valid_dense, t_valid_dense = roc_curve(y_valid, y_valid_preds_dense)\n",
    "auc_valid_dense = roc_auc_score(y_valid, y_valid_preds_dense)\n",
    "\n",
    "fpr_valid_lstm, tpr_valid_lstm, t_valid_lstm = roc_curve(y_valid, y_valid_preds_lstm)\n",
    "auc_valid_lstm = roc_auc_score(y_valid, y_valid_preds_lstm)\n",
    "\n",
    "plt.plot(fpr_valid_cnn, tpr_valid_cnn, 'g-', label = 'CNN AUC:%.3f'%auc_valid_cnn)\n",
    "plt.plot(fpr_valid_dense, tpr_valid_dense, 'r-', label = 'Dense AUC:%.3f'%auc_valid_dense)\n",
    "plt.plot(fpr_valid_lstm, tpr_valid_lstm, 'b-', label = 'LSTM AUC:%.3f'%auc_valid_lstm)\n",
    "\n",
    "plt.plot([0,1],[0,1], 'k--')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.legend(bbox_to_anchor = (1.04,1), loc = 'upper left')\n",
    "plt.title('Validation Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
